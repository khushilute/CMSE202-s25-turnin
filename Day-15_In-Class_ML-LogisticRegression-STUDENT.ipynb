{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Assignment: Machine Learning; classification with logistic regression\n",
    "# Day 15\n",
    "# CMSE 202\n",
    "<img src=\"https://i.ibb.co/vv36b1d/supervised-workflow-machine-learning.png\" width=700px>\n",
    "\n",
    "\n",
    "### Agenda for today's class\n",
    "\n",
    "</p>\n",
    "\n",
    "1. [Review of Pre-Class assignment](#review)\n",
    "1. [Training vs Testing](#train-test)\n",
    "1. [Logistic Regression](#logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; **Khushi Lute** </p>\n",
    "#### <p style=\"text-align: right;\"> &#9989; Mary, jamie, Kate, Sam, Ryleigh</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"review\"></a>\n",
    "## 1. Review of Pre-Class assignment\n",
    "\n",
    "We'll discussion any questions that came up as a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"train-test\"></a>\n",
    "## 2. Training vs Testing\n",
    "\n",
    "As you learned in the pre-class, classification is an ML process that maps features of an input data set to class labels. Classification is a **supervised** learning approach where example data is used to train the data. We typically divide the data used to train and evaluate the classifier (the result model) into three sets\n",
    "\n",
    "- training set\n",
    "- testing set\n",
    "- validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do This:** As a group, discuss what these three sets represent. It might help to review these terms on the web. Put your answers down below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Training set is: the portion of the data used to train the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Testing set is:The testing set is a separate portion of the data that is used to evaluate the performance of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Validation set is:The validation set is used during the training process to tune the model's hyperparameters and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the features and building the model\n",
    "\n",
    "If you review the image at the top of the notebook, you might notice that one of the first steps in machine learning is to go from \"raw data\" into a set of \"features\" and \"labels\". Extracting features from our data can sometimes be one of the trickier parts of the process and also one of the most important ones. We have to think carefully about exactly what the \"right\" features are for training our machine learning algorithm and, when possible, it is advantageous to find ways to reduce the total number of features we are trying to model. Once we define our features, we can build our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Working with data\n",
    "\n",
    "There is a common data set used to work with classification called the breast cancer data set. It is actually available in `sklearn` but rather than working with the dataset that's been cleaned up for us, let's continue to flex your data wrangling skills and look at the original data. You'll need to download two data files:\n",
    "\n",
    "* `breast-cancer-wisconsin.data`\n",
    "* `breast-cancer-wisconsin.names`\n",
    "\n",
    "The data are in \".data\" and the \".names\" describes that data. \n",
    "\n",
    "You can download the files from here:\n",
    "\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-supplemental-data/main/data/breast-cancer-wisconsin.data`\n",
    "\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-supplemental-data/main/data/breast-cancer-wisconsin.names`\n",
    "\n",
    "&#9989; **Do This:** Read in the data, label the columns based on the .names file. Look at the dtypes, anything unusual? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> What's unusual about dtypes? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -O https://raw.githubusercontent.com/msu-cmse-courses/cmse202-supplemental-data/main/data/breast-cancer-wisconsin.names\n",
    "curl -O https://raw.githubusercontent.com/msu-cmse-courses/cmse202-supplemental-data/main/data/breast-cancer-wisconsin.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do This:** Can you identify what the problem is? If so, can you provide a DataFrame with just the rows that are causing the problem? There are lots of ways to do this so talk it out with your group. If you get stuck, talk with an instructor, so that you can move on to the next part of the assignment without spending too much time here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate column names found: ['-- Size of data set', '-- Size of data set', '#####', '#####', '#####', '#####']\n",
      "Problematic rows with missing or unusual values:\n",
      "     Citation Request  acknowledgements.  Also, please cite one or more of  \\\n",
      "0             1000025                                                  5     \n",
      "1             1002945                                                  5     \n",
      "2             1015425                                                  3     \n",
      "3             1016277                                                  6     \n",
      "4             1017023                                                  4     \n",
      "..                ...                                                ...     \n",
      "694            776715                                                  3     \n",
      "695            841769                                                  2     \n",
      "696            888820                                                  5     \n",
      "697            897471                                                  4     \n",
      "698            897471                                                  4     \n",
      "\n",
      "     1. O. L. Mangasarian and W. H. Wolberg  \\\n",
      "0                                         1   \n",
      "1                                         4   \n",
      "2                                         1   \n",
      "3                                         8   \n",
      "4                                         1   \n",
      "..                                      ...   \n",
      "694                                       1   \n",
      "695                                       1   \n",
      "696                                      10   \n",
      "697                                       8   \n",
      "698                                       8   \n",
      "\n",
      "     2. William H. Wolberg and O.L. Mangasarian  \\\n",
      "0                                             1   \n",
      "1                                             4   \n",
      "2                                             1   \n",
      "3                                             8   \n",
      "4                                             1   \n",
      "..                                          ...   \n",
      "694                                           1   \n",
      "695                                           1   \n",
      "696                                          10   \n",
      "697                                           6   \n",
      "698                                           8   \n",
      "\n",
      "     3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg  \\\n",
      "0                                                    1    \n",
      "1                                                    5    \n",
      "2                                                    1    \n",
      "3                                                    1    \n",
      "4                                                    3    \n",
      "..                                                 ...    \n",
      "694                                                  1    \n",
      "695                                                  1    \n",
      "696                                                  3    \n",
      "697                                                  4    \n",
      "698                                                  5    \n",
      "\n",
      "     via linear programming  in  4. K. P. Bennett & O. L. Mangasarian  \\\n",
      "0                         2   1                                     3   \n",
      "1                         7  10                                     3   \n",
      "2                         2   2                                     3   \n",
      "3                         3   4                                     3   \n",
      "4                         2   1                                     3   \n",
      "..                      ...  ..                                   ...   \n",
      "694                       3   2                                     1   \n",
      "695                       2   1                                     1   \n",
      "696                       7   3                                     8   \n",
      "697                       3   4                                    10   \n",
      "698                       4   5                                    10   \n",
      "\n",
      "     1. Title  2. Sources  ...  #####_41  #####_42  5. Number of Instances  \\\n",
      "0           1           1  ...       NaN       NaN                     NaN   \n",
      "1           2           1  ...       NaN       NaN                     NaN   \n",
      "2           1           1  ...       NaN       NaN                     NaN   \n",
      "3           7           1  ...       NaN       NaN                     NaN   \n",
      "4           1           1  ...       NaN       NaN                     NaN   \n",
      "..        ...         ...  ...       ...       ...                     ...   \n",
      "694         1           1  ...       NaN       NaN                     NaN   \n",
      "695         1           1  ...       NaN       NaN                     NaN   \n",
      "696        10           2  ...       NaN       NaN                     NaN   \n",
      "697         6           1  ...       NaN       NaN                     NaN   \n",
      "698         4           1  ...       NaN       NaN                     NaN   \n",
      "\n",
      "     6. Number of Attributes  7. Attribute Information  11. Class  \\\n",
      "0                        NaN                       NaN        NaN   \n",
      "1                        NaN                       NaN        NaN   \n",
      "2                        NaN                       NaN        NaN   \n",
      "3                        NaN                       NaN        NaN   \n",
      "4                        NaN                       NaN        NaN   \n",
      "..                       ...                       ...        ...   \n",
      "694                      NaN                       NaN        NaN   \n",
      "695                      NaN                       NaN        NaN   \n",
      "696                      NaN                       NaN        NaN   \n",
      "697                      NaN                       NaN        NaN   \n",
      "698                      NaN                       NaN        NaN   \n",
      "\n",
      "     8. Missing attribute values  9. Class distribution  Benign  Malignant  \n",
      "0                            NaN                    NaN     NaN        NaN  \n",
      "1                            NaN                    NaN     NaN        NaN  \n",
      "2                            NaN                    NaN     NaN        NaN  \n",
      "3                            NaN                    NaN     NaN        NaN  \n",
      "4                            NaN                    NaN     NaN        NaN  \n",
      "..                           ...                    ...     ...        ...  \n",
      "694                          NaN                    NaN     NaN        NaN  \n",
      "695                          NaN                    NaN     NaN        NaN  \n",
      "696                          NaN                    NaN     NaN        NaN  \n",
      "697                          NaN                    NaN     NaN        NaN  \n",
      "698                          NaN                    NaN     NaN        NaN  \n",
      "\n",
      "[699 rows x 51 columns]\n",
      "\n",
      "Data types of each column:\n",
      "Citation Request                                                         int64\n",
      "acknowledgements.  Also, please cite one or more of                      int64\n",
      "1. O. L. Mangasarian and W. H. Wolberg                                   int64\n",
      "2. William H. Wolberg and O.L. Mangasarian                               int64\n",
      "3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg                       int64\n",
      "via linear programming                                                   int64\n",
      "in                                                                      object\n",
      "4. K. P. Bennett & O. L. Mangasarian                                     int64\n",
      "1. Title                                                                 int64\n",
      "2. Sources                                                               int64\n",
      "-- Donor                                                                 int64\n",
      "-- Date                                                                float64\n",
      "3. Past Usage                                                          float64\n",
      "Each instance has one of 2 possible classes                            float64\n",
      "-- Size of data set                                                    float64\n",
      "-- Collected classification results                                    float64\n",
      "-- Accuracy on remaining 50% of dataset                                float64\n",
      "-- Accuracy on remaining 33% of dataset                                float64\n",
      "Learning Conference} (pp. 470--479).  Aberdeen, Scotland               float64\n",
      "-- Size of data set_19                                                 float64\n",
      "-- Best accuracy result                                                float64\n",
      "-- 1-nearest neighbor                                                  float64\n",
      "-- Also of interest                                                    float64\n",
      "-- Using only typical instances                                        float64\n",
      "4. Relevant Information                                                float64\n",
      "from the data itself                                                   float64\n",
      "Group 1                                                                float64\n",
      "Group 2                                                                float64\n",
      "Group 3                                                                float64\n",
      "Group 4                                                                float64\n",
      "Group 5                                                                float64\n",
      "Group 6                                                                float64\n",
      "Group 7                                                                float64\n",
      "Group 8                                                                float64\n",
      "Total                                                                  float64\n",
      "statements summarizes changes to the original Group 1's set of data    float64\n",
      "#####  Group 1                                                         float64\n",
      "#####  Revised Jan 10, 1991                                            float64\n",
      "#####  Revised Nov 22,1991                                             float64\n",
      "#####                                                                  float64\n",
      "#####_40                                                               float64\n",
      "#####_41                                                               float64\n",
      "#####_42                                                               float64\n",
      "5. Number of Instances                                                 float64\n",
      "6. Number of Attributes                                                float64\n",
      "7. Attribute Information                                               float64\n",
      "11. Class                                                              float64\n",
      "8. Missing attribute values                                            float64\n",
      "9. Class distribution                                                  float64\n",
      "Benign                                                                 float64\n",
      "Malignant                                                              float64\n",
      "dtype: object\n",
      "\n",
      "Clean DataFrame (rows with missing values removed):\n",
      "Empty DataFrame\n",
      "Columns: [Citation Request, acknowledgements.  Also, please cite one or more of, 1. O. L. Mangasarian and W. H. Wolberg, 2. William H. Wolberg and O.L. Mangasarian, 3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg, via linear programming, in, 4. K. P. Bennett & O. L. Mangasarian, 1. Title, 2. Sources, -- Donor, -- Date, 3. Past Usage, Each instance has one of 2 possible classes, -- Size of data set, -- Collected classification results, -- Accuracy on remaining 50% of dataset, -- Accuracy on remaining 33% of dataset, Learning Conference} (pp. 470--479).  Aberdeen, Scotland, -- Size of data set_19, -- Best accuracy result, -- 1-nearest neighbor, -- Also of interest, -- Using only typical instances, 4. Relevant Information, from the data itself, Group 1, Group 2, Group 3, Group 4, Group 5, Group 6, Group 7, Group 8, Total, statements summarizes changes to the original Group 1's set of data, #####  Group 1, #####  Revised Jan 10, 1991, #####  Revised Nov 22,1991, #####, #####_40, #####_41, #####_42, 5. Number of Instances, 6. Number of Attributes, 7. Attribute Information, 11. Class, 8. Missing attribute values, 9. Class distribution, Benign, Malignant]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('breast-cancer-wisconsin.names', 'r') as names_file:\n",
    "    column_names = [line.split(':')[0].strip() for line in names_file.readlines() if ':' in line]\n",
    "duplicate_names = [name for name in column_names if column_names.count(name) > 1]\n",
    "print(\"Duplicate column names found:\", duplicate_names)\n",
    "seen = set()\n",
    "for i in range(len(column_names)):\n",
    "    if column_names[i] in seen:\n",
    "        column_names[i] = f\"{column_names[i]}_{i}\"  # Append an index to make unique\n",
    "    seen.add(column_names[i])\n",
    "\n",
    "data = pd.read_csv('breast-cancer-wisconsin.data', header=None, names=column_names)\n",
    "\n",
    "data.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "problematic_rows = data[data.isna().any(axis=1)]\n",
    "\n",
    "print(\"Problematic rows with missing or unusual values:\")\n",
    "print(problematic_rows)\n",
    "\n",
    "print(\"\\nData types of each column:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "clean_data = data.dropna()\n",
    "\n",
    "print(\"\\nClean DataFrame (rows with missing values removed):\")\n",
    "print(clean_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you hopefully discussed, we happen to have some rows with missing data values. So, as we've seen previously, we have an imputation problem.\n",
    "\n",
    "&#9989; **Do This:**  Write code to solve this missing data problem and say what you did.\n",
    "\n",
    "By the way, there is an argument `na_values` that you can provide to `read_csv` that will mark a list of characters as if they were `np.nan` using `na_values`, which is pretty darn convenient. Using that will help when importing the data for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate column names found: ['-- Size of data set', '-- Size of data set', '#####', '#####', '#####', '#####']\n",
      "\n",
      "Missing data overview:\n",
      "Citation Request                                                         0\n",
      "acknowledgements.  Also, please cite one or more of                      0\n",
      "1. O. L. Mangasarian and W. H. Wolberg                                   0\n",
      "2. William H. Wolberg and O.L. Mangasarian                               0\n",
      "3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg                       0\n",
      "via linear programming                                                   0\n",
      "in                                                                      16\n",
      "4. K. P. Bennett & O. L. Mangasarian                                     0\n",
      "1. Title                                                                 0\n",
      "2. Sources                                                               0\n",
      "-- Donor                                                                 0\n",
      "-- Date                                                                699\n",
      "3. Past Usage                                                          699\n",
      "Each instance has one of 2 possible classes                            699\n",
      "-- Size of data set                                                    699\n",
      "-- Collected classification results                                    699\n",
      "-- Accuracy on remaining 50% of dataset                                699\n",
      "-- Accuracy on remaining 33% of dataset                                699\n",
      "Learning Conference} (pp. 470--479).  Aberdeen, Scotland               699\n",
      "-- Size of data set_19                                                 699\n",
      "-- Best accuracy result                                                699\n",
      "-- 1-nearest neighbor                                                  699\n",
      "-- Also of interest                                                    699\n",
      "-- Using only typical instances                                        699\n",
      "4. Relevant Information                                                699\n",
      "from the data itself                                                   699\n",
      "Group 1                                                                699\n",
      "Group 2                                                                699\n",
      "Group 3                                                                699\n",
      "Group 4                                                                699\n",
      "Group 5                                                                699\n",
      "Group 6                                                                699\n",
      "Group 7                                                                699\n",
      "Group 8                                                                699\n",
      "Total                                                                  699\n",
      "statements summarizes changes to the original Group 1's set of data    699\n",
      "#####  Group 1                                                         699\n",
      "#####  Revised Jan 10, 1991                                            699\n",
      "#####  Revised Nov 22,1991                                             699\n",
      "#####                                                                  699\n",
      "#####_40                                                               699\n",
      "#####_41                                                               699\n",
      "#####_42                                                               699\n",
      "5. Number of Instances                                                 699\n",
      "6. Number of Attributes                                                699\n",
      "7. Attribute Information                                               699\n",
      "11. Class                                                              699\n",
      "8. Missing attribute values                                            699\n",
      "9. Class distribution                                                  699\n",
      "Benign                                                                 699\n",
      "Malignant                                                              699\n",
      "dtype: int64\n",
      "\n",
      "Missing data after imputation:\n",
      "Citation Request                                                         0\n",
      "acknowledgements.  Also, please cite one or more of                      0\n",
      "1. O. L. Mangasarian and W. H. Wolberg                                   0\n",
      "2. William H. Wolberg and O.L. Mangasarian                               0\n",
      "3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg                       0\n",
      "via linear programming                                                   0\n",
      "in                                                                       0\n",
      "4. K. P. Bennett & O. L. Mangasarian                                     0\n",
      "1. Title                                                                 0\n",
      "2. Sources                                                               0\n",
      "-- Donor                                                                 0\n",
      "-- Date                                                                699\n",
      "3. Past Usage                                                          699\n",
      "Each instance has one of 2 possible classes                            699\n",
      "-- Size of data set                                                    699\n",
      "-- Collected classification results                                    699\n",
      "-- Accuracy on remaining 50% of dataset                                699\n",
      "-- Accuracy on remaining 33% of dataset                                699\n",
      "Learning Conference} (pp. 470--479).  Aberdeen, Scotland               699\n",
      "-- Size of data set_19                                                 699\n",
      "-- Best accuracy result                                                699\n",
      "-- 1-nearest neighbor                                                  699\n",
      "-- Also of interest                                                    699\n",
      "-- Using only typical instances                                        699\n",
      "4. Relevant Information                                                699\n",
      "from the data itself                                                   699\n",
      "Group 1                                                                699\n",
      "Group 2                                                                699\n",
      "Group 3                                                                699\n",
      "Group 4                                                                699\n",
      "Group 5                                                                699\n",
      "Group 6                                                                699\n",
      "Group 7                                                                699\n",
      "Group 8                                                                699\n",
      "Total                                                                  699\n",
      "statements summarizes changes to the original Group 1's set of data    699\n",
      "#####  Group 1                                                         699\n",
      "#####  Revised Jan 10, 1991                                            699\n",
      "#####  Revised Nov 22,1991                                             699\n",
      "#####                                                                  699\n",
      "#####_40                                                               699\n",
      "#####_41                                                               699\n",
      "#####_42                                                               699\n",
      "5. Number of Instances                                                 699\n",
      "6. Number of Attributes                                                699\n",
      "7. Attribute Information                                               699\n",
      "11. Class                                                              699\n",
      "8. Missing attribute values                                            699\n",
      "9. Class distribution                                                  699\n",
      "Benign                                                                 699\n",
      "Malignant                                                              699\n",
      "dtype: int64\n",
      "\n",
      "Cleaned data:\n",
      "   Citation Request  acknowledgements.  Also, please cite one or more of  \\\n",
      "0           1000025                                                  5     \n",
      "1           1002945                                                  5     \n",
      "2           1015425                                                  3     \n",
      "3           1016277                                                  6     \n",
      "4           1017023                                                  4     \n",
      "\n",
      "   1. O. L. Mangasarian and W. H. Wolberg  \\\n",
      "0                                       1   \n",
      "1                                       4   \n",
      "2                                       1   \n",
      "3                                       8   \n",
      "4                                       1   \n",
      "\n",
      "   2. William H. Wolberg and O.L. Mangasarian  \\\n",
      "0                                           1   \n",
      "1                                           4   \n",
      "2                                           1   \n",
      "3                                           8   \n",
      "4                                           1   \n",
      "\n",
      "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg  via linear programming  \\\n",
      "0                                                  1                        2   \n",
      "1                                                  5                        7   \n",
      "2                                                  1                        2   \n",
      "3                                                  1                        3   \n",
      "4                                                  3                        2   \n",
      "\n",
      "     in  4. K. P. Bennett & O. L. Mangasarian  1. Title  2. Sources  ...  \\\n",
      "0   1.0                                     3         1           1  ...   \n",
      "1  10.0                                     3         2           1  ...   \n",
      "2   2.0                                     3         1           1  ...   \n",
      "3   4.0                                     3         7           1  ...   \n",
      "4   1.0                                     3         1           1  ...   \n",
      "\n",
      "   #####_41  #####_42  5. Number of Instances  6. Number of Attributes  \\\n",
      "0       NaN       NaN                     NaN                      NaN   \n",
      "1       NaN       NaN                     NaN                      NaN   \n",
      "2       NaN       NaN                     NaN                      NaN   \n",
      "3       NaN       NaN                     NaN                      NaN   \n",
      "4       NaN       NaN                     NaN                      NaN   \n",
      "\n",
      "   7. Attribute Information  11. Class  8. Missing attribute values  \\\n",
      "0                       NaN        NaN                          NaN   \n",
      "1                       NaN        NaN                          NaN   \n",
      "2                       NaN        NaN                          NaN   \n",
      "3                       NaN        NaN                          NaN   \n",
      "4                       NaN        NaN                          NaN   \n",
      "\n",
      "   9. Class distribution  Benign  Malignant  \n",
      "0                    NaN     NaN        NaN  \n",
      "1                    NaN     NaN        NaN  \n",
      "2                    NaN     NaN        NaN  \n",
      "3                    NaN     NaN        NaN  \n",
      "4                    NaN     NaN        NaN  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3887208/3418545026.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "with open('breast-cancer-wisconsin.names', 'r') as names_file:\n",
    "    column_names = [line.split(':')[0].strip() for line in names_file.readlines() if ':' in line]\n",
    "\n",
    "duplicate_names = [name for name in column_names if column_names.count(name) > 1]\n",
    "print(\"Duplicate column names found:\", duplicate_names)\n",
    "\n",
    "seen = set()\n",
    "for i in range(len(column_names)):\n",
    "    if column_names[i] in seen:\n",
    "        column_names[i] = f\"{column_names[i]}_{i}\"  # Append an index to make unique\n",
    "    seen.add(column_names[i])\n",
    "\n",
    "data = pd.read_csv('breast-cancer-wisconsin.data', header=None, names=column_names, na_values='?')\n",
    "\n",
    "print(\"\\nMissing data overview:\")\n",
    "print(data.isna().sum())  \n",
    "\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    data[col].fillna(data[col].mean(), inplace=True)\n",
    "\n",
    "categorical_cols = data.select_dtypes(include=[object]).columns\n",
    "for col in categorical_cols:\n",
    "    data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "\n",
    "print(\"\\nMissing data after imputation:\")\n",
    "print(data.isna().sum())  \n",
    "\n",
    "print(\"\\nCleaned data:\")\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 2.2 : Splitting the dataset for model into training and testing sets\n",
    "Now that's we've loaded up and cleaned up the data, let's split the data in a training set and final testing set. We want to randomly select 75% of the data for training and 25% of the data for testing.\n",
    "\n",
    "Also, **you should turn the `class_labels` into 0 (currently 2, for benign) and 1 (currently 4, for malignant) as the classifier we are going to use (Logisitic Regression) predicts valuse between 0 and 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do This**: You will need to come up with a way to split the data into separate training and testing sets (we will leave the validation set out for now).  Make sure you keep the feature vectors and classes together.  \n",
    "\n",
    "**BIG HINT**: This is a very common step in machine learning, and there exists a function to do this for you in the `sklearn` library called `train_test_split`. From the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), you find that takes the features and class labels as input and returns\n",
    "4 outputs:\n",
    "- 2 feature sets (one for training and one for testing)\n",
    "- 2 class labels sets (the corresponding one for training and for testing)\n",
    "\n",
    "Use `train_test_split` to split your data into a training set and a testing set that correspond to 75% and 25% of your data respectively. To ensure that some of the provided code below will work, you should use the variable names `train_vectors`, `test_vectors`, `train_labels`, and `test_labels` to store your results. Check the length of the resulting output to make sure the splits follow what you expected.\n",
    "\n",
    "**Important Note**: You'll need to break up your dataframe into a set of labels and a set of features before you do the train-test splitting. You also want to make sure you \"features\" don't include any columns that don't make sense to use as features (i.e. avoid including columns that would not be expected to have any meaningful influence on the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 524\n",
      "Testing set size: 175\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data.iloc[:, :-1]  \n",
    "labels = data.iloc[:, -1]    \n",
    "\n",
    "labels = labels.replace({2: 0, 4: 1})\n",
    "\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(features, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_vectors)}\")\n",
    "print(f\"Testing set size: {len(test_vectors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question**: Why do we need to separate our samples into a training and testing set. Why can't we just use all the data for both? Wouldn't that make it work better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> We separate the data into training and testing sets to prevent overfitting. if we use all the data for both training and testing ,it wouldn't generalize well to new, unseen data. The testing set helps to evaluate the model's realworld performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"logit\"></a>\n",
    "## 3 Logistic Regression\n",
    "\n",
    "In the pre-class, you watched a video explaining some of the aspects of the logistic regression. The full details on logistic regression require deeper study, but we can gain some insight from looking at the function we are trying to fit to our data. Plot out the curve to the following equation, called the **logistic function**:\n",
    "\n",
    "$$ f(x) = \\frac{e^{x}} {1+e^{x} }  \\equiv \\frac{1}{1+e^{-x} } $$\n",
    "\n",
    "&#9989; **Do This**: Create a logistic function and then make the plot of $f(x)$ for $x$ over the range -6 to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def logistic_function(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = logistic_function(x)\n",
    "\n",
    "# Plot the logistic function\n",
    "plt.plot(x, y, label='Logistic Function', color='b')\n",
    "plt.title('Logistic Function Curve')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='black',linewidth=0.5)\n",
    "plt.axvline(0, color='black',linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is interesting about that curve is that all values of x are mapped into the range for y of 0.0-1.0. Assuming you have a binary classifier, that is one that only has two class labels, that is the mapping you want: all combination of features map into the two class labels 0, or 1. Moreover, the graph looks a lot like a [cumulative probability distribution](https://en.wikipedia.org/wiki/Cumulative_distribution_function). The probability that a set of features is of class 1 is 1 to the right and 0 to the left. As you watched in the pre-class video, this is the basis for logistic regression.\n",
    "\n",
    "It's considered a regression because the \"x\" in our logisitic function is actually going to be a regression equation, such that\n",
    "\n",
    "$$ fn= b_{0} + b_{1}x_{1} + b_{2}x_{2} + \\ldots  $$\n",
    "\n",
    "for as many terms as we like and the new logistic function\n",
    "\n",
    "$$ f(x) = \\frac{e^{fn(x))}} {1+e^{fn(x)} }  \\equiv \\frac{1}{1+e^{-fn(x)} } $$\n",
    "\n",
    "Logisitic classification tries to find the values for the parameters $b_{i}$ that gives maximal performance on training, and hopefully testing. Let's let `statsmodels` do that.\n",
    "\n",
    "We are going to use all the training data from above and train a logistic regression. It is similar to what we did before with regular regression.\n",
    "\n",
    "Note, very importantly, the use of `sm.add_constant` on the training vectors. We talked about that when we did OLS in statsmodels. That column of constant is what the $b_{0}$ or intercept will train against. We need that column to get an intercept. (Note that this code requires that you used `train_labels` and `train_vectors` when you did the train-test split earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2475983244.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    train_vectors_with_const_cleaned =\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Pseudo R-squ\" is the equivalent (mostly) of the R-squared value in Linear regression that we looked for before. It ranges from 0 (poor fit) to 1 (perfect fit). The P values under \"P > |z|\" are measures of significance. The null hypothesis is that the restricted model (say a constant value for `fn`) performs better and a low p-value suggests that we can reject this hypothesis and prefer the full model over the null model (I.e., LOW $P$-VALUE IS GOOD). This is similar to the F-test for linear regression.\n",
    "\n",
    "&#9989; **Do This**: Based on the results from above, remove the low-performing columns and then recreate the training and testing sets and run it again. Display the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question:** How do the fits of the full model and the reduced model compare? What evidence are you using to make compare these two fits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Do This - Erase the contents of this cell and replace it with your answer to the above question!  (double-click on this text to edit this cell, and hit shift+enter to save the text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 How'd it go?\n",
    "\n",
    "There are a number of ways that we can check the performance of our model and we will continue new ways throughout the semester. The major difference in the standard statistics approach and supervised learning approaches is that we test our models using the data that we held out: \"the testing data.\" \n",
    "\n",
    "That is, we will use our classifier model to make predictions from the test features and we can then compare those predictions to actual test labels. To test accuracy, we can use the output of the `.fit()` method of the model to predict how well the classifier works on the test data (the data it was not trained on). Conveniently that is the `.predict()` method and, again, we use it on the result of the `.fit()`. \n",
    "\n",
    "**Note:** The output from `.predict()` is not a 0/1 value as the test labels are, but rather a fraction between 0 and 1 indicating how likely each entry is to be one class or another. We can make the assumption that anything greater than 0.5 would be a 1 class and anything less than 0.5 would be a 0 class. \n",
    "\n",
    "&#9989; **Do This**: do the following:\n",
    "- use the `.predict()` method (look up the documentation as necessary) to create the predicted labels using the test input\n",
    "- convert the output of the `.predict()` method to the 0/1 class values of the test labels\n",
    "- print the resulting predicted class values of the test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first metrics we will use in determining how well a machine learning model is working is the \"accuracy score\", which compares the predictions our model made for the test labels and the actual test labels. This score is one of many metrics we can use and is included in `sklearn.metrics` as (surprise) `accuracy_score()`. Here's the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) on `accuracy_score`.\n",
    "\n",
    "&#9989; **Do This**: Using the predicted 0/1 labels you just created:\n",
    "- Use the `sklearn.metrics` we imported at the top and run the `accuracy_score` on the 0/1 predicted label and the test labels.\n",
    "- Print your accuracy result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question:** How well did your model predict the test class labels? Given what you learned in the pre-class assignment about false positives and false negatives, what other questions should we ask about the accuracy of our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Do This - Erase the contents of this cell and replace it with your answer to the above question!  (double-click on this text to edit this cell, and hit shift+enter to save the text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!\n",
    "\n",
    "Now, you just need to submit this assignment by uploading it to the course <a href=\"https://d2l.msu.edu/\">Desire2Learn</a> web page for today's submission folder (Don't forget to add your names in the first cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2024,  Department of Computational Mathematics, Science and Engineering at Michigan State University\n",
    "\n",
    "<!-- 9/16/2024 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.11 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
